# Transformer

1. [**On Layer Normalization IN The Transformer A Rchitecture**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/LayerNormTransformer.pdf) [-] _Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu_.
   - the reason of post-LN Transformer needing warm-up process in theoretically and empirically.
   - The Position of Layer Normalization in residual connections is significant.
   - $\left\|\frac{\partial \tilde{\mathcal{L}}}{\partial W^{2, L}}\right\|_{F} \leq \mathcal{O}(d \sqrt{\ln d})$
   - $\left\|\frac{\partial \tilde{\mathcal{L}}}{\partial W^{2, L}}\right\|_{F} \leq \mathcal{O}(d \sqrt{\frac{\ln d}{L}})$
2. [**Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/MacaronNet.pdf) [-] _Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun,Bin Dong, Tao Qin, Liwei Wang, Tie-Yan Liu_.
   - Transformer is Ordinary Differential Equation ODE
   - And change Transformer architecture to FFN-Attention-FFN
3. [**Large Memory Layers with Product Keys**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/LargeMemoryLayers.pdf) [-] _Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, HervÃ© JÃ©gou_.
   - a network about key-value memory layer instead of FC in Transformer
   - It can reduce computational complexity.
4. [**A Tensorized Transformer for Language Modeling**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/TensorizedTransformer.pdf) [NIPS 2019] _Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Dawei Song, Ming Zhou_.
   - parameters sharing + tensor decomposition
   - block-term tensor decomposition(BTD)
   - Multi-linear Attention by BTD
   - reduce computational complexity
5. [**Adaptive Attention Span in Transformers**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/AdaptiveAttSpanInTransformer.pdf) [ACL 2019] _Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin_.
   - adaptive attention span size reduce unnecessary cost.
   - origin Attention span $\in [t-S, t)$
   - In fact, diff head have diff focus. Maybe focus on near info, maybe on global info.
   - proposal a masking function to control attention span.
   - $m_{z}(x)=\min \left[\max \left[\frac{1}{R}(R+z-x), 0\right], 1\right]$
   - $a_{t r}=\frac{m_{z}(t-r) \exp \left(s_{t r}\right)}{\sum_{q=t-S}^{t-1} m_{z}(t-q) \exp \left(s_{t q}\right)}$
6. [**Universal Transformers**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/UniversalTransformers.pdf) [ICLR 2019] _Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Åukasz Kaiser_.
   - Recurrent Architectures -> time loop
   - Transition functions -> share parameters
   - adaptive computational time -> dynamic adjust step time
7. [**Depth-Adaptive Transformer**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/DepthAdaptiveTransformer.pdf) [ICLR 2020] _Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli_.
   - depth adaptive Transformer
   - not enough every token run -> last layer
   - $$\forall n, p\left(y_{t+1} | h_{t}^{n}\right)=\operatorname{softmax}\left(W_{n} h_{t}^{n}\right)$$
   - disadvantages: O(N), classification sole.
   - origin method: dynamic calculate
     - Aligned Training: know 1~i-1 layers all hidden parameters
       - $$\mathrm{LL}_{t}^{n}=\log p\left(y_{t} | h_{t-1}^{n}\right), \quad \mathrm{LL}^{n}=\sum_{t=1}^{|\boldsymbol{y}|} \mathrm{LL}_{t}^{n}, \quad \mathcal{L}_{d e c}(\boldsymbol{x}, \boldsymbol{y})=-\frac{1}{\sum_{n} \omega_{n}} \sum_{n=1}^{N} \omega_{n} \mathrm{LL}^{n}$$
     - Mixed Training: i+1 copy i layer
       - $$\mathrm{LL}_{t}=\log p\left(y_{t} | h_{t-1}^{n}\right), \quad \mathrm{LL}=\sum_{t=1}^{|\boldsymbol{y}|} \mathrm{LL}_{t}, \quad \mathcal{L}_{\operatorname{dec}}(\boldsymbol{x}, \boldsymbol{y})=-\frac{1}{M} \sum_{m=1}^{M} \mathrm{LL}$$
   - Paper Method: Depth-Adaptive
     - Sequence-specific depth: the same sequence quit same time
       - likelihood-based
         - $$q^{*}(\boldsymbol{x}, \boldsymbol{y})=\delta\left(\arg \max _{n} \mathrm{LL}^{n}-\lambda n\right)$$
       - correctness-based
         - $$C^{n}=\#\left\{t\left|y_{t}=\arg \max _{y} p\left(y | h_{t-1}^{n}\right\}, \quad q^{*}(\boldsymbol{x}, \boldsymbol{y})=\delta\left(\arg \max _{n} C^{n}-\lambda n\right)\right.\right.$$
     - token-specific depth
       - Multinomial
       - Poisson Binomial
       - likelihood-based
         - $$\begin{array}{l}{\kappa\left(t, t^{\prime}\right)=e^{-\frac{\left|t-t^{\prime}\right|^{2}}{\sigma}}, \text { smoothLL }_{t}^{n}=\sum_{t^{\prime}} \kappa\left(t, t^{\prime}\right) \mathrm{LL}_{t^{\prime}}^{n}} \\ {q_{t}^{*}(\boldsymbol{x}, \boldsymbol{y})=\delta\left(\arg \max _{n} \operatorname{smooth} \mathrm{LL} \mathrm{L}_{t}^{n}-\lambda n\right)}\end{array}$$
       - correctness-based
         - $$\begin{array}{l}{C_{t}^{n}=y_{t}=\arg \max _{y} p\left(y | h_{t-1}^{n}\right), \operatorname{smooth} \mathrm{C}_{t}^{n}=\sum_{t^{\prime}} \kappa\left(t, t^{\prime}\right) C_{t}^{n}} \\ {q_{t}^{*}(\boldsymbol{x}, \boldsymbol{y})=\delta\left(\arg \max _{n} \operatorname{smooth} \mathrm{C}_{t}^{n}-\lambda n\right)}\end{array}$$
       - confidence threshold
8. [**Star-Transformer**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/StarTransformer.pdf) [NAACL 2019] _Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, Zheng Zhang_.
   - change the FFN architecture of Transformer to Star architecture
   - reduce the compute cost and the dataSet need.
9. [**Single Headed Attention RNN: Stop Thinking With Your Head**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/SHA-RNN.pdf) [-] _Stephen Merity_.
   - Paper style is like a blog.
   - LSTM + Att + FFN(save memory)
   - get completable result.
10. [**Longformer: The Long-Document Transformer**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/Longformer.pdf) [-] _Iz Beltagy and Matthew E. Peters and Arman Cohan_.
    - Global + Slide Windows Attention.
    - Cuda TVM to speed up.

## Relative position embedding

1. [**Self-Attention with Relative Position Representations**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/RelativePEAtt.pdf) [NAACL 2018] _Peter Shaw, Jakob Uszkoreit, Ashish Vaswani_.
   - 18 å¹´çš„ NAACL(é‚£å°±æ˜¯ 17 å¹´åº•çš„å·¥ä½œ)ï¼Œæ–‡ç« æ˜¯ä¸€ç¯‡çŸ­æ–‡ï¼Œ_Peter Shaw, Jakob Uszkoreit, Ashish Vaswani_ çœ‹åå­—æ˜¯å‘ Transformer çš„é‚£æ‰¹äººï¼ˆæƒ³æ¥å…¶ä»–äººä¹Ÿä¸èƒ½åœ¨é‚£ä¹ˆçŸ­æ—¶é—´æœ‰é‚£ä¹ˆæ·±çš„æ€è€ƒ ğŸ¤”ï¼‰.
   - ä»–ä»¬åˆ†åˆ«åœ¨ QK ä¹˜ç§¯è®¡ç®— Attention bias çš„æ—¶å€™å’Œ SoftMax ä¹‹ååœ¨ Value åé¢ä¸¤å¤„åœ°æ–¹åŠ ä¸Šäº†ä¸€ä¸ªç›¸å¯¹ç¼–ç (ä¸¤å¤„å‚æ•°ä¸å…±äº«)ã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577650515531-b28795fb-b793-4599-b964-bed869b0e34a.png)
   - ä¸ºäº†é™ä½å¤æ‚åº¦ï¼Œåœ¨ä¸åŒ head ä¹‹é—´å…±äº«äº†å‚æ•°ã€‚
   - å®éªŒæ˜¾ç¤ºï¼Œåœ¨ WMT14 è‹±å¾·æ•°æ®é›†ä¸Š base model BLEU æå‡äº† 0.3, big model æå‡äº† 1.3.
   - Ablation å®éªŒä¸­ï¼Œæ”¹å˜æœ€å¤§ä½ç½®è·ç¦» kï¼Œæ˜¾ç¤º k ä» 0-4 å¢å¤§çš„è¿‡ç¨‹ performance æœ‰æ˜æ˜¾çš„æå‡ï¼Œä¹‹åå†å¢å¤§ k æå‡ä¸æ˜æ˜¾ã€‚
   - Attention bias ä¸­çš„ç›¸å¯¹é¡¹æé«˜æ›´å¤šçš„ performance, è€Œ SoftMax ä¹‹åå† Value ä¸ŠåŠ çš„é‚£ä¸ªç›¸å¯¹é¡¹æå‡çš„æ€§èƒ½ç•¥å°‘ã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648873593-abd42821-6a14-4e07-aa2c-95559109e13c.png)
2. [**Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/Transformer-XL.pdf) [ACL 2019] _Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov_.
   - Transformer-XL çš„å·¥ä½œç°åœ¨å¯ä»¥ç§°ä¹‹ä¸Šå¼€åˆ›æ€§çš„äº†ã€‚
   - é™¤å´é€’å½’æ›´æ–° blank çš„è¿‡ç¨‹ï¼Œrelative position embedding ä¹Ÿæ˜¯è®ºæ–‡çš„ä¸€å¤§äº®ç‚¹.
   - å¯èƒ½ä½œè€…åœ¨æ€è€ƒçš„è¿‡ç¨‹ä¸­æ›´å¤šæ˜¯åœ¨ recurrent æ˜¯ PE é‡å é€ æˆçš„åç§»è§’åº¦å‡ºå‘ã€‚
   - ä½†å®é™…ä¸Š relative çš„æ”¹åŠ¨å¯¹äºæ¨¡å‹è·å¾—å¥å†…ç»†ç²’åº¦å±‚æ¬¡çš„ä¿¡æ¯ä¹Ÿæ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577650361230-3fe5fa1f-953a-4428-a4c5-bff76c128554.png)
   - ä¸ NAACL18 é‚£ç¯‡ä¸åŒçš„åœ°æ–¹ï¼ŒTransformer-XL èˆå¼ƒäº†åœ¨ SoftMax ä¹‹åå†å åŠ  Rijã€‚
   - å¦å¤–æŠŠ Attention Bias ä¸­ï¼Œå¦å¤–ä¸¤é¡¹å› ä¸ºå¼•å…¥ PE äº§ç”Ÿçš„è¡¨å¾ç»å¯¹ä½ç½®ä¿¡æ¯çš„ä¸¤é¡¹æ›¿æ¢æˆä¸åŒ…å«ä½ç½®ä¿¡æ¯çš„ä¸€ç»´å‘é‡(è¿™é‡Œä¸“é—¨ä¸ºå‰é¢ recurrent æ¨¡å¼è®¾è®¡çš„ï¼Œæ„Ÿè§‰å¦‚æœä¸æ­é… Transformer-XL ä½¿ç”¨çš„è¯è¿™ä¸ªæ”¹åŠ¨å¹¶ä¸ä¸€å®šæ˜¯æœ€åˆé€‚çš„)ã€‚
   - é™¤æ­¤ä¹‹å¤–ï¼Œä¸ºäº†é¿å…ç›¸å¯¹å¼•å…¥çš„å·¨å¤§è®¡ç®—é‡ï¼Œåˆ©ç”¨ç±»ä¼¼ AES ä¸­çš„ shift æ“ä½œå¯ä»¥æŠŠå¤æ‚åº¦é™åˆ°çº¿æ€§ã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648875063-0c613a26-13a6-4b50-bb8f-fb236d82f3e3.png)
   - é€šè¿‡ Ablation å®éªŒå¯ä»¥çœ‹å‡ºç›¸å¯¹äº NAACL18 çš„æ”¹è¿›èƒ½æ˜¾è‘—æå‡æ•ˆæœã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577650364187-bef17d51-131c-40aa-ba48-7aee6a8db06f.png)
   - å¦å¤–é€šè¿‡ RECL å®éªŒä¹Ÿèƒ½çœ‹å‡º relative PE å¯¹æ¨¡å‹çš„é•¿ç¨‹èƒ½åŠ›æœ‰æ‰€å¸®åŠ©
   - RECL æ˜¯ä¸€ä¸ªé€¼è¿‘å®éªŒï¼Œé€šè¿‡æµ‹é‡ä¸Šä¸‹æ–‡é•¿åº¦ä¸º c + â–³ï¼Œç›¸å¯¹é•¿åº¦ä¸º c æ—¶æœ€å° loss çš„å˜åŒ–ç‡ã€‚
   - å½“å˜åŒ–ç‡ä½äºä¸€ä¸ªé˜ˆå€¼çš„æ—¶å€™å°±è¯´æ˜å¤§äºé•¿åº¦ c çš„ä¿¡æ¯å¯¹æ¨¡å‹ performance æå‡å¸®åŠ©ä¸å¤§ã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577649253152-4c73f333-edd8-4e05-abe6-30d582043a09.png)
3. [**Encoding word order in complex embeddings**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Transformer/encoding_word_order_in_complex_embeddings.pdf) [ICLR 2020] _Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, Jakob Grue Simonsen_.
   - é™¤äº†ä¸Šé¢ä¸€ç³»åˆ—ä» Attention bias å‡ºå‘çš„è§’åº¦ï¼Œè¿˜æœ‰ dalao ä» PE ä¸ WE ç»“åˆæ–¹å¼è§’åº¦å‡ºå‘åšçš„å·¥ä½œã€‚
   - ä»æŠ½è±¡å±‚æ¬¡çœ‹ï¼Œå‰é¢çš„æ–¹æ³•éƒ½æ˜¯åœ¨è¡¥æ•‘å› ä¸ºï¼ˆWE+PEï¼‰ä¹˜ç§¯é¡¹é€ æˆçš„ä¸¢å¤±ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚
   - å¦‚æœ WE å’Œ PE çš„ç»“åˆæ–¹å¼ä¸æ˜¯åŠ æ€§å‘¢ã€‚
   - ä»Šå¹´ ICLR çš„æœ‰ä¸€ç¯‡å·¥ä½œå°±æ˜¯ä»è¿™ä¸ªè§’åº¦å‡ºå‘ï¼Œå°† WE äº PE ç»„åˆåˆ†è§£æˆç‹¬ç«‹çš„è¿ç»­å‡½æ•°ã€‚
   - è¿™æ ·åœ¨ä¹‹åçš„ Attention Bias è®¡ç®—æ—¶ä¹Ÿä¸ä¼šä¸¢å¤± Position ç›¸å¯¹ä¿¡æ¯ã€‚
   - ä¸ºäº†è¾¾æˆè¿™ä¸ªç›®çš„ï¼Œå°±éœ€è¦æ‰¾åˆ°ä¸€ç§å˜æ¢ï¼Œä½¿å¾—å¯¹äºä»»æ„ä½ç½® posï¼Œéƒ½æœ‰$g(pos+n) = \text{Transform}_n(g(pos))$, ä¸ºäº†é™ä½éš¾åº¦æŠŠæ ‡å‡†é™ä½æˆæ‰¾åˆ°ä¸€ç§çº¿æ€§å˜æ¢ Transformã€‚
   - è€Œæˆ‘ä»¬çš„ Embed é™¤äº†ä¸Šé¢çš„æ€§è´¨ä¹‹å¤–åº”è¯¥è¿˜æ˜¯æœ‰ç•Œçš„ã€‚
   - è¿™ç¯‡æ–‡ç« è¯æ˜åœ¨æ»¡è¶³ä¸Šè¿°æ¡ä»¶ä¸‹ï¼ŒTransform çš„å”¯ä¸€è§£æ˜¯å¤æ•°åŸŸä¸­çš„$g(pos)=z_{2} z_{1}^{pos}$, ä¸” z1 çš„å¹…å€¼å°äº 1ã€‚
   - ï¼ˆè¿™ä¸ªè¯æ˜ä¹Ÿæ˜¯æœ‰ã€ç®€å•ï¼Œreviewer çš„è¯´æ³•å°±æ˜¯æœ‰ã€ä¼˜ç¾
   - æ ¹æ®æ¬§æ‹‰å…¬å¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¯¹ä¸Šè¿°å¼å­è¿›è¡ŒåŒ–ç®€
   - $g(\text { pos })=z_{2} z_{1}^{\text {pos }}=r_{2} e^{i \theta_{2}}\left(r_{1} e^{i \theta_{1}}\right)^{\text {pos }}=r_{2} r_{1}^{\text {pos }} e^{i\left(\theta_{2}+\theta_{1} \text { pos }\right)}$
   - ä¸ºäº†å·æ‡’ï¼ŒæŠŠ r1 è®¾æˆ 1, è€Œ$e^{ix}$çš„å¹…å€¼ç­‰äº 1ï¼Œå°±æ’æ»¡è¶³ r1 çš„é™å®šã€‚
   - äºæ˜¯ï¼Œè¿›ä¸€æ­¥åŒ–ç®€ä¸º $g(\mathrm{pos})=r e^{i(\omega \mathrm{pos}+\theta)}$
   - è¿™å°±æ˜¯æ ‡æ ‡å‡†å‡†çš„è™šå•ä½åœ†çš„å½¢å¼ï¼Œr ä¸ºåŠå¾„ï¼Œ$\theta$ä¸ºåˆå§‹å¹…è§’ï¼Œ$\frac{\omega}{2\pi}$ ä¸ºé¢‘ç‡ï¼Œé€†æ—¶é’ˆæ—‹è½¬ã€‚
   - è¿™ä¸ªå¼å­åˆå¯ä»¥åŒ–æˆ$f(j, \text { pos })=g_{w e}(j) \odot g_{p e}(j, \text { pos })$ WE ä¸ PE çš„å¤šé¡¹å¼ä¹˜ç§¯ï¼Œå…¶ä¸­ä¸¤è€…æ‰€å ç³»æ•°å–å†³äºå­¦ä¹ åˆ°çš„ç³»æ•°ã€‚
   - äºæ˜¯è¿™ç›¸å½“äºä¸€ä¸ªè‡ªé€‚åº”çš„è°ƒèŠ‚ WE å’Œ PE å æ¯”çš„æ¨¡å¼ã€‚
   - å®é™…ä¸Šæˆ‘ä»¬åªéœ€è¦å»å­¦ä¹ å¹…å€¼ r, é¢‘ç‡ wï¼Œåˆå§‹å¹…è§’$\theta$ (ä¼šé€ æˆå‚æ•°é‡ç•¥å¾®å¢å¤§)
   - å¯ä»¥çœ‹å‡º Vanilla Transformer ä¸­çš„ PE æ˜¯ä¸Šå¼çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ã€‚
   - ä¸ºäº†é€‚åº” Embedding æ‹“å±•åˆ°å¤æ•°åŸŸï¼ŒRNNï¼ŒLSTM, Transformer çš„è®¡ç®—ä¹Ÿåº”è¯¥è¦æ‹“å±•åˆ°å¤æ•°åŸŸ.
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648877715-21485bde-7aa8-412f-9b9d-1bd6f6d603da.png)
   - å®éªŒéƒ¨åˆ†åšäº† Text Classification, MT, LM ä¸‰ä¸ª taskã€‚
   - Text Classification é€‰äº† 4 ä¸ª sentiment analysis æ•°æ®é›†ï¼Œä¸€ä¸ªä¸»è§‚å®¢è§‚åˆ†ç±»ï¼Œä¸€ä¸ªé—®é¢˜åˆ†ç±»ï¼Œå…±å…­ä¸ª benchmarkã€‚
   - Baseline è®¾ç½® 1. without PE; 2. random PE and train; 3. ä¸‰è§’ PE; 4. r ç”± pretrain åˆå§‹åŒ–ï¼Œw éšæœºåˆå§‹åŒ–$(-\pi, \pi)$; 5. r ç”± pretrain åˆå§‹åŒ–ï¼Œw train;
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648879630-4b3f1e39-cb94-4946-b140-267207022da5.png)
   - å¯ä»¥çœ‹å‡º performance ä¸­ä¸‰è§’ PE ä¸ Train PE å‡ ä¹æ²¡ä»€ä¹ˆåŒºåˆ«ï¼Œæ‰€ä»¥ Vanilla Transformer ä½¿ç”¨ Fix çš„ PE ä¹Ÿæ˜¯æœ‰ä¸€å®šé“ç†çš„ã€‚
   - Complex order å¯¹æ¨¡å‹æœ‰ä¸€å®šæå‡ï¼Œä½†ä¸æ˜¯ç‰¹åˆ«å¤šã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648881717-c065aabc-520c-41b2-a9ca-6335e61a70d9.png)
   - ç„¶åä¸ºäº†é™ä½å‚æ•°é‡ï¼Œå°è¯•äº†å‡ ç§å‚æ•°å…±äº«çš„ç»„åˆ
   - ï¼ˆå‚æ•°é‡çš„å¢å¤§ä¸»è¦æ˜¯å› ä¸º Transformer ç»“æ„æ‰©å……åˆ°å¤æ•°åŸŸï¼Œå‚æ•°é‡å¢å¤§äº†ä¸€å€ã€‚ä½†å‚æ•°å¢å¤§ä¸ prefermance å…³è”åº¦ä¸å¤§ï¼Œå…±äº« W ä¹‹åæ€§èƒ½å‡ ä¹ä¸å˜ã€‚
   - ç„¶ååœ¨ WMT16 è‹±å¾·å’Œ text8 ä¸Šæµ‹äº†åœ¨ MT å’Œ LM ä¸Šçš„æ€§èƒ½ï¼Œè¿™ä¸¤éƒ¨åˆ†æ€§èƒ½æå‡è¿˜æ˜¯å¾ˆæ˜æ˜¾çš„ã€‚å°¤å…¶æ˜¯ LM åœ¨åŒç­‰å‚æ•°é‡ä¸‹çš„å¯¹æ¯”è¯•éªŒã€‚
   - ![image](https://cdn.nlark.com/yuque/0/2019/png/104214/1577648883560-364bafa1-455d-430e-bfe4-a4246c3e0e15.png)
   - (è¿™ç¯‡çš„ä½œè€…ä¹‹å‰ä¹Ÿå‘äº†å‡ ç¯‡å…³äºå¤æ•°åŸŸä¸Š NLP çš„åº”ç”¨ï¼Œä¹‹å‰æ¨¡å‹çš„åå­—ä¹Ÿå¾ˆæœ‰æ„æ€ ä»€ä¹ˆ [CNM](https://www.aclweb.org/anthology/N19-1420/)çš„ å¾ˆçœŸå®
