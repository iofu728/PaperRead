# Attention

1. [**Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Attention/AnalysisMultiHeadAttention.pdf) [ACL 2019] _Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov_.
   - analysis which head is most effect in MT
   - only a small subset of heads are important for translation.
   - Important Heads have one or more specialized and interpretable functions.
