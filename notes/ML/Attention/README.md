# Attention

1. [**Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Attention/AnalysisMultiHeadAttention.pdf) [ACL 2019] _Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov_.
   - analysis which head is most effect in MT
   - only a small subset of heads are important for translation.
   - Important Heads have one or more specialized and interpretable functions.
2. [**Attention is not Explanation**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Attention/AttentionNotExplanation.pdf) [NAACL 2019] _Sarthak Jain, Byron C. Wallace_.
   - Attention weighted weakly correction with feature weighted.
   - even same alternative attention can change the prediction.
