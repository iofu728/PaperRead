# Dropout

1. [**Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Dropout/Mixout.pdf) [ICLR 2020] _Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang_.
   - finetune on large-scale dataset to reduce overfiting.
   - Origin + Dropout
   - proof the random mixture function have one lower bound.
2. [**Do We Need Zero Training Loss AÂ‰er Achieving Zero Training Error?**](https://github.com/iofu728/PaperRead/blob/master/paper/ML/Dropout/flood.pdf) [ICML 2020] _Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama_.
   - Give a constant loss when overfit which can give more momentum to make model give up locally minimum.
