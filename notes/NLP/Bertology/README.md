# Bertology

1. [**ALBERT: A L ITE BERT FOR S ELF - SUPERVISED L EARNING OF L ANGUAGE R EPRESENTATIONS**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/ALBert.pdf)[ICLR 2020] _Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut_.

   - ÊÑüËßâ‰∏ãÊù• ALBert Êõ¥ÂÉèÊòØ‰∏ÄÁØáË∞ÉÂèÇÂÆûÈ™åÊä•Âëä üçã
   - Êï¥‰∏™Â∑•‰ΩúÁöÑ Motivation ÊòØÂáèÂ∞ëÂèÇÊï∞ÈáèÔºåÂä†ÈÄüËÆ≠ÁªÉÊó∂Èó¥ÔºåÊâÄ‰ª• performance Êéâ‰∏ÄÁÇπÈóÆÈ¢ò‰∏çÂ§ß
   - Èô§‰∫Ü‰∏âÁÇπÊúÄÂ§ßÁöÑÊîπËøõ HighLight
     - ÂáèÂ∞è Embedding sizeÔºå‰ΩøÂÖ∂‰∏çÂÜç‰∏é Transformer ÁöÑ Hidden Size Áõ∏Á≠â
     - ÂÖ±‰∫´ Transformer Âíå FFN ÁöÑÂèÇÊï∞ÔºàËøô‰∏çÊòØÂú®ÂºÄ Multi-head ÁöÑÂÄíËΩ¶Âêó
     - ÊîπÂèò NSP negative sample ÁöÑÊûÑÈÄ†ÊñπÊ≥ï
   - Ëøò follow ‰∫Ü‰∏Ä‰∫õ‰πãÂâç Bertology Ê®°ÂûãÁöÑ tricks
     - spanBert ÁöÑ n-gram masking
     - ‰πãÂâç 76min ËÆ≠ÁªÉ Bert ÈÇ£ÁØáÁî®Âà∞ÁöÑ LAMB ‰ºòÂåñÂô®
   - ÂØπ‰∫éÂáèÂ∞è embed size Ëøô‰∏ÄÁÇπÔºåÂÖ∂ÂÆûÂú®Âπ≥Êó∂ÂÆûÈ™åËøáÁ®ã‰∏≠‰πüÊ∑±ÊúâÊÑüËß¶ÔºåÂú®Êúâ‰∫õ Task ‰∏äÈÇ£ bert ÁöÑ Embedding Â±ÇÂá∫Êù•Êé•ÂÖ∂‰ªñ modelÔºåÊïàÊûúÂèØËÉΩËøò‰ºöÊØî Glove Â∑Æ„ÄÇÂ∞§ÂÖ∂ÊòØÂΩìÁî® bert-large ÁöÑÊó∂ÂÄôÔºåÈÇ£ embed size ÂèØÊòØÊúâ 1024„ÄÇ‰∏ªË¶ÅËøòÊòØ size Â§™Â§ßÔºåÂ§™ËøáÁ®ÄÁñèÔºåËÄåÂáèÂ∞è‰∫Ü‰∏ªË¶ÅÁöÑ feature Áª¥Â∫¶ÁöÑË¥°ÁåÆ„ÄÇ
   - ÂÖ±‰∫´ÂèÇÊï∞Â§ßÊ¶ÇÂçïÁ∫ØÂè™ÊòØÊÉ≥ÂáèÂ∞èÂèÇÊï∞Áª¥Â∫¶ÂêßÔºåÊØïÁ´ü multi-head ÁöÑÊúâÊïàÊÄßËøòÊòØÊëÜÂú®ÈÇ£ÈáåÁöÑ„ÄÇ
   - Ëá≥‰∫éÁ¨¨‰∏âÁÇπÔºåÂ§ßÊ¶Ç‰πüÊòØÈöèÊú∫ÈÄâÊã© negative sample ÁöÑ‰∏Ä‰∏™ÈÄöÁóÖÂêß„ÄÇÂú®ËøôÈáåÁªôÂá∫ÁöÑËß£ÈáäÊòØ‰πãÂâç random negative sample ÂÆπÊòìËÆ©Ê®°ÂûãÂ≠¶‰π†Âà∞‰∏ªÈ¢òÂåπÈÖçËøô‰∏™ÊØîËæÉÁÆÄÂçïÁöÑ‰ªªÂä°ÂéªÔºåËÄåÂøΩÁï•Êõ¥Ê∑±Â±ÇÁöÑËØ≠‰πâËøûË¥ØÊÄß„ÄÇ
   - Ëá≥‰∫éÊ®°ÂûãÂèÇÊï∞ÈáèÈôç‰∏ãÊù•‰πãÂêéÂë¢ÔºåÂΩìÁÑ∂ÊòØÁªßÁª≠Â¢ûÂä†Ê®°ÂûãÂèÇÊï∞Âï¶Ôºà ÁãóÂ§¥
   - Ëá≥‰∫é bert-xlarge ‰∏ç workÔºå ËÄå ALBert-xlarge workÔºåÊàëÁöÑÊÑüËßâÊòØÈ¶ñÂÖà Embed Â±ÇÁª¥Â∫¶Ë¢´ÊîæÂ§ßÂ§™Â§ö‰∫ÜÔºåÂØºËá¥‰ø°ÊÅØËøá‰∫éÂêë‰∏é‰∏ä‰∏ãÊñáÂÅèÁßªÂêß. ËÄåÂ¢ûÂä† Hidden size ‰ºö‰ΩøÂæóÈù†ÂêéÁöÑ layer ‰ø°ÊÅØËøõ‰∏ÄÊ≠•Ë¢´ÊîæÂ§ß. ËÄå ALBer-xlarge È¶ñÂÖàÂáèÂ∞è Embed Áª¥Â∫¶ËøáÂ§ßÁöÑÂΩ±Âìç, ÁÑ∂Âêé share ‰∫ÜÊâÄÊúâÂ±ÇÁöÑÂèÇÊï∞, Áõ∏ÂΩì‰∫éÊääÂéüÊù•ÊâÄÊúâ layer ÂÅö‰∫Ü‰∏™Âπ≥Âùá(ËÄå‰∏çÊòØÂè™ÂèñÊúÄÂêé‰∏ÄÂ±Ç hidden layer ÁöÑËæìÂá∫. ËøôÊ†∑ÂÆûÈôÖ‰∏ä‰ºöÂâäÂº±Ëøá‰∫éÂÉèÊõ¥È´òÂ±ÇËØ≠‰πâ‰ø°ÊÅØÂÅèÁßªÁöÑË∂ãÂäø. ÊÑüËßâÂ¶ÇÊûú‰ΩúËÄÖÂÅö‰∫Ü ALBert-xlarge ÁöÑ no-share ÂÆûÈ™åÂ∫îËØ•‰ºöÊúâÊâÄ‰ΩìÁé∞.
   - ËÄå ALBert-xxlarge ÊØî ALBert-xlarge Êõ¥ work, ÊÑüËßâ‰πüÊòØÂ∑Æ‰∏çÂ§öÁöÑÂéüÂõ†. Â¢ûÂä† Hidden size ÊòØÊâ©ÂÖÖÈù†ÂêéÂ±ÇÁ∫ßËé∑ÂæóÁöÑ semantic ‰ø°ÊÅØ. ‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÅö‰∫ÜÂèÇÊï∞ÂÖ±‰∫´,ÂÆûÈôÖ‰∏ä‰ΩúÁî®Âà∞ËæìÂá∫ÁöÑÊòØ layer ÁöÑÊüêÁßçÂπ≥Âùá. ALBert-xlarge ÊØî ALbert-large work, ALBert-xxlarge ÊØî ALbert-xlarge work ËØ¥ÊòéÂØπ ALBert ËøôÁßç Language Model ÊâÄËï¥Âê´ÁöÑ‰ø°ÊÅØÈáèËøòËÉΩÈÄöËøáÊîæÂ§ß hidden size Êâ©ÂÖÖ.( ËøòÊòØËßâÂæóÂ∞ë‰∫Ü‰∏ÄÁªÑ 12 layer 2048 hidden size ÁöÑÂÆûÈ™å, Áé∞Âú®ËøôÁßçÊÉÖÂÜµÂè™ËÉΩÁõ≤Áåú
   - ÊÄªÁöÑÊù•ËØ¥ÔºåËøôÊòØ‰∏ÄÁØá Betrology ÈõÜÂ§ßÊàêÁöÑ paperÔºåÊñáÁ´†ÈÄªËæëÂÆûÈ™åÈÉΩÂÜôÁöÑ‰∏çÈîôÔºåÂåÖÊã¨‰∏Ä‰∫õ idea ÁöÑÊèêÂá∫ÔºåÂèÇËÄÉÊñáÁåÆÁöÑÈÄâÊã©ÔºåËøòÊòØÂª∫ËÆÆ‰∏ÄÁúãÔºåËÉΩÂä†Ê∑±ÂØπ Bertology ÁöÑÁêÜËß£„ÄÇÔºàËØùËØ¥ refer ÈáåÈù¢ËøòÊúâ 10 Â§öÁØáÊ≤°ÁúãËøá Ë¶ÅÂéªÈù¢Â£Å‰∫Ü

2. [**Unified Language Model Pre-training for Natural Language Understanding and Generation**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/UniLM.pdf) [-] _Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon_.
   - three mask
     - unidirectional
     - bidirectional
     - seq2seq
   - framework support both NLU & NLG
3. [**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/T5.pdf) [-] _Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu._
   - Natural Language Decathlon.
   - every task -> seq2seq.
     - (TaskName: Sentences) -> (Task result)
     - except regression task -> (1, 1.2, 1.4 ... 5) multi-class classification
   - C4 Colossal Clean Crawled Corpus. by some heuristic method.
   - using teacher forcing + cross-entropy loss.
   - AdaFactor Optimizer + greedy decoding.
   - pack multi seq -> each entity of one batch.
   - inverse square root warm-up strategy $1/sqrt(max(n, k))$
   - SentencePrice -> 32000 vocab
   - like UnifiedLM Architectures -> Prefix LM
   - Test for multi corruption rate & corrupted span lens.
4. [**BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/BART.pdf) [-] _Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer_.
   - Auto-ENcode + Auto-Regressive
   - add more noisy to text
   - first, corrupting text with an arbitrary noising function.
   - second, reconstruct the original text.
   - Token Mask + Token Deletion + Text Infilling + Sentence Permutation + Document Rotation
   - LM + Permutation LM + MLM + Multitask MLM + Masked Seq-seq
   - good at NLG
5. [**ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators **](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/ELECTRA.pdf) [ICLR 2020] _Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning_.
   - Motivation: more difficult MASK
   - a little smaller MLM -> Generate random MASK -> a result
   - a discriminator if the result is correct
   - Weight Sharing -> sharing embedding & weight
   - Smaller generators
   - Training Algorithm: frozen.
6. [**Reformer: The Efficient Transformer**](https://github.com/iofu728/PaperRead/bl/master/paper/NLP/Bertology/Reformer.pdf) [ICLR 2020] _Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya_.
   - LSH Mask Attention:
     - Motivation: One token should only focus on one of the few tokens.
   - indecency chunking FFN
   - single reversible copy
7. [**MPNet: Masked and Permuted Pre-training for Language Understanding**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/MPNet.pdf) [-] _Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu_.
   - MLM + PLM.
   - split prediction and no-prediction.
   - make [MASK] can see other have predicted [MASK] .(condition)
   - make [MASK] can see others position.
8. [**MC-BERT: Efficient Language Pre-Training via a Meta Controller**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/MC-BERT.pdf) [-] _Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, Tie-Yan Liu_.
   - Probing why ELECTRA is good?
     - higher sample efÔ¨Åciency. ELECTRA-simple
     - reduced task complexity. ELECTRA-complexity(which is significantly, but why MC-BERT add the complexity, conciliatory?)
   - ELECTRA not good at semantic task.
   - K choice

## Multi-modality Bert

1. [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/CodeBERT.pdf) [-] _Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou_.
   - PL + NL
   - MLM + RDT(Replaced Token Detection)

## BERT Downstream

1. [**Multi-Stage Document Ranking with BERT**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Rank/DuoBert.pdf) [-] _Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin_.
   - Recall-Ranking.
   - Multi-stage add pairwise information.
