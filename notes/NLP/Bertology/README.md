# Bertology

1. [ALBERT: A L ITE BERT FOR S ELF - SUPERVISED L EARNING OF L ANGUAGE R EPRESENTATIONS](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/ALBert.pdf)[submit ICLR 2020] Anonymous authors.

   - æ„Ÿè§‰ä¸‹æ¥ ALBert æ›´åƒæ˜¯ä¸€ç¯‡è°ƒå‚å®éªŒæŠ¥å‘Š ğŸ‹
   - æ•´ä¸ªå·¥ä½œçš„ Motivation æ˜¯å‡å°‘å‚æ•°é‡ï¼ŒåŠ é€Ÿè®­ç»ƒæ—¶é—´ï¼Œæ‰€ä»¥ performance æ‰ä¸€ç‚¹é—®é¢˜ä¸å¤§
   - é™¤äº†ä¸‰ç‚¹æœ€å¤§çš„æ”¹è¿› HighLight
     - å‡å° Embedding sizeï¼Œä½¿å…¶ä¸å†ä¸ Transformer çš„ Hidden Size ç›¸ç­‰
     - å…±äº« Transformer å’Œ FFN çš„å‚æ•°ï¼ˆè¿™ä¸æ˜¯åœ¨å¼€ Multi-head çš„å€’è½¦å—
     - æ”¹å˜ NSP negative sample çš„æ„é€ æ–¹æ³•
   - è¿˜ follow äº†ä¸€äº›ä¹‹å‰ Bertology æ¨¡å‹çš„ tricks
     - spanBert çš„ n-gram masking
     - ä¹‹å‰ 76min è®­ç»ƒ Bert é‚£ç¯‡ç”¨åˆ°çš„ LAMB ä¼˜åŒ–å™¨
   - å¯¹äºå‡å° embed size è¿™ä¸€ç‚¹ï¼Œå…¶å®åœ¨å¹³æ—¶å®éªŒè¿‡ç¨‹ä¸­ä¹Ÿæ·±æœ‰æ„Ÿè§¦ï¼Œåœ¨æœ‰äº› Task ä¸Šé‚£ bert çš„ Embedding å±‚å‡ºæ¥æ¥å…¶ä»– modelï¼Œæ•ˆæœå¯èƒ½è¿˜ä¼šæ¯” Glove å·®ã€‚å°¤å…¶æ˜¯å½“ç”¨ bert-large çš„æ—¶å€™ï¼Œé‚£ embed size å¯æ˜¯æœ‰ 1024ã€‚ä¸»è¦è¿˜æ˜¯ size å¤ªå¤§ï¼Œå¤ªè¿‡ç¨€ç–ï¼Œè€Œå‡å°äº†ä¸»è¦çš„ feature ç»´åº¦çš„è´¡çŒ®ã€‚
   - å…±äº«å‚æ•°å¤§æ¦‚å•çº¯åªæ˜¯æƒ³å‡å°å‚æ•°ç»´åº¦å§ï¼Œæ¯•ç«Ÿ multi-head çš„æœ‰æ•ˆæ€§è¿˜æ˜¯æ‘†åœ¨é‚£é‡Œçš„ã€‚
   - è‡³äºç¬¬ä¸‰ç‚¹ï¼Œå¤§æ¦‚ä¹Ÿæ˜¯éšæœºé€‰æ‹© negative sample çš„ä¸€ä¸ªé€šç—…å§ã€‚åœ¨è¿™é‡Œç»™å‡ºçš„è§£é‡Šæ˜¯ä¹‹å‰ random negative sample å®¹æ˜“è®©æ¨¡å‹å­¦ä¹ åˆ°ä¸»é¢˜åŒ¹é…è¿™ä¸ªæ¯”è¾ƒç®€å•çš„ä»»åŠ¡å»ï¼Œè€Œå¿½ç•¥æ›´æ·±å±‚çš„è¯­ä¹‰è¿è´¯æ€§ã€‚
   - è‡³äºæ¨¡å‹å‚æ•°é‡é™ä¸‹æ¥ä¹‹åå‘¢ï¼Œå½“ç„¶æ˜¯ç»§ç»­å¢åŠ æ¨¡å‹å‚æ•°å•¦ï¼ˆ ç‹—å¤´
   - è‡³äº bert-xlarge ä¸ workï¼Œ è€Œ ALBert-xlarge workï¼Œæˆ‘çš„æ„Ÿè§‰æ˜¯é¦–å…ˆ Embed å±‚ç»´åº¦è¢«æ”¾å¤§å¤ªå¤šäº†ï¼Œå¯¼è‡´ä¿¡æ¯è¿‡äºå‘ä¸ä¸Šä¸‹æ–‡åç§»å§. è€Œå¢åŠ  Hidden size ä¼šä½¿å¾—é åçš„ layer ä¿¡æ¯è¿›ä¸€æ­¥è¢«æ”¾å¤§. è€Œ ALBer-xlarge é¦–å…ˆå‡å° Embed ç»´åº¦è¿‡å¤§çš„å½±å“, ç„¶å share äº†æ‰€æœ‰å±‚çš„å‚æ•°, ç›¸å½“äºæŠŠåŸæ¥æ‰€æœ‰ layer åšäº†ä¸ªå¹³å‡(è€Œä¸æ˜¯åªå–æœ€åä¸€å±‚ hidden layer çš„è¾“å‡º. è¿™æ ·å®é™…ä¸Šä¼šå‰Šå¼±è¿‡äºåƒæ›´é«˜å±‚è¯­ä¹‰ä¿¡æ¯åç§»çš„è¶‹åŠ¿. æ„Ÿè§‰å¦‚æœä½œè€…åšäº† ALBert-xlarge çš„ no-share å®éªŒåº”è¯¥ä¼šæœ‰æ‰€ä½“ç°.
   - è€Œ ALBert-xxlarge æ¯” ALBert-xlarge æ›´ work, æ„Ÿè§‰ä¹Ÿæ˜¯å·®ä¸å¤šçš„åŸå› . å¢åŠ  Hidden size æ˜¯æ‰©å……é åå±‚çº§è·å¾—çš„ semantic ä¿¡æ¯. ä¸»è¦æ˜¯å› ä¸ºåšäº†å‚æ•°å…±äº«,å®é™…ä¸Šä½œç”¨åˆ°è¾“å‡ºçš„æ˜¯ layer çš„æŸç§å¹³å‡. ALBert-xlarge æ¯” ALbert-large work, ALBert-xxlarge æ¯” ALbert-xlarge work è¯´æ˜å¯¹ ALBert è¿™ç§ Language Model æ‰€è•´å«çš„ä¿¡æ¯é‡è¿˜èƒ½é€šè¿‡æ”¾å¤§ hidden size æ‰©å…….( è¿˜æ˜¯è§‰å¾—å°‘äº†ä¸€ç»„ 12 layer 2048 hidden size çš„å®éªŒ, ç°åœ¨è¿™ç§æƒ…å†µåªèƒ½ç›²çŒœ
   - æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ç¯‡ Betrology é›†å¤§æˆçš„ paperï¼Œæ–‡ç« é€»è¾‘å®éªŒéƒ½å†™çš„ä¸é”™ï¼ŒåŒ…æ‹¬ä¸€äº› idea çš„æå‡ºï¼Œå‚è€ƒæ–‡çŒ®çš„é€‰æ‹©ï¼Œè¿˜æ˜¯å»ºè®®ä¸€çœ‹ï¼Œèƒ½åŠ æ·±å¯¹ Bertology çš„ç†è§£ã€‚ï¼ˆè¯è¯´ refer é‡Œé¢è¿˜æœ‰ 10 å¤šç¯‡æ²¡çœ‹è¿‡ è¦å»é¢å£äº†

2. [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/UniLM.pdf) [-] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon.
   - three mask
     - unidirectional
     - bidirectional
     - seq2seq
   - framework support both NLU & NLG
