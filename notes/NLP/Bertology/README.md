# Bertology

1. [**ALBERT: A L ITE BERT FOR S ELF - SUPERVISED L EARNING OF L ANGUAGE R EPRESENTATIONS**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/ALBert.pdf)[ICLR 2020] _Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut_.

   - æ„Ÿè§‰ä¸‹æ¥ ALBert æ›´åƒæ˜¯ä¸€ç¯‡è°ƒå‚å®žéªŒæŠ¥å‘Š ðŸ‹
   - æ•´ä¸ªå·¥ä½œçš„ Motivation æ˜¯å‡å°‘å‚æ•°é‡ï¼ŒåŠ é€Ÿè®­ç»ƒæ—¶é—´ï¼Œæ‰€ä»¥ performance æŽ‰ä¸€ç‚¹é—®é¢˜ä¸å¤§
   - é™¤äº†ä¸‰ç‚¹æœ€å¤§çš„æ”¹è¿› HighLight
     - å‡å° Embedding sizeï¼Œä½¿å…¶ä¸å†ä¸Ž Transformer çš„ Hidden Size ç›¸ç­‰
     - å…±äº« Transformer å’Œ FFN çš„å‚æ•°ï¼ˆè¿™ä¸æ˜¯åœ¨å¼€ Multi-head çš„å€’è½¦å—
     - æ”¹å˜ NSP negative sample çš„æž„é€ æ–¹æ³•
   - è¿˜ follow äº†ä¸€äº›ä¹‹å‰ Bertology æ¨¡åž‹çš„ tricks
     - spanBert çš„ n-gram masking
     - ä¹‹å‰ 76min è®­ç»ƒ Bert é‚£ç¯‡ç”¨åˆ°çš„ LAMB ä¼˜åŒ–å™¨
   - å¯¹äºŽå‡å° embed size è¿™ä¸€ç‚¹ï¼Œå…¶å®žåœ¨å¹³æ—¶å®žéªŒè¿‡ç¨‹ä¸­ä¹Ÿæ·±æœ‰æ„Ÿè§¦ï¼Œåœ¨æœ‰äº› Task ä¸Šé‚£ bert çš„ Embedding å±‚å‡ºæ¥æŽ¥å…¶ä»– modelï¼Œæ•ˆæžœå¯èƒ½è¿˜ä¼šæ¯” Glove å·®ã€‚å°¤å…¶æ˜¯å½“ç”¨ bert-large çš„æ—¶å€™ï¼Œé‚£ embed size å¯æ˜¯æœ‰ 1024ã€‚ä¸»è¦è¿˜æ˜¯ size å¤ªå¤§ï¼Œå¤ªè¿‡ç¨€ç–ï¼Œè€Œå‡å°äº†ä¸»è¦çš„ feature ç»´åº¦çš„è´¡çŒ®ã€‚
   - å…±äº«å‚æ•°å¤§æ¦‚å•çº¯åªæ˜¯æƒ³å‡å°å‚æ•°ç»´åº¦å§ï¼Œæ¯•ç«Ÿ multi-head çš„æœ‰æ•ˆæ€§è¿˜æ˜¯æ‘†åœ¨é‚£é‡Œçš„ã€‚
   - è‡³äºŽç¬¬ä¸‰ç‚¹ï¼Œå¤§æ¦‚ä¹Ÿæ˜¯éšæœºé€‰æ‹© negative sample çš„ä¸€ä¸ªé€šç—…å§ã€‚åœ¨è¿™é‡Œç»™å‡ºçš„è§£é‡Šæ˜¯ä¹‹å‰ random negative sample å®¹æ˜“è®©æ¨¡åž‹å­¦ä¹ åˆ°ä¸»é¢˜åŒ¹é…è¿™ä¸ªæ¯”è¾ƒç®€å•çš„ä»»åŠ¡åŽ»ï¼Œè€Œå¿½ç•¥æ›´æ·±å±‚çš„è¯­ä¹‰è¿žè´¯æ€§ã€‚
   - è‡³äºŽæ¨¡åž‹å‚æ•°é‡é™ä¸‹æ¥ä¹‹åŽå‘¢ï¼Œå½“ç„¶æ˜¯ç»§ç»­å¢žåŠ æ¨¡åž‹å‚æ•°å•¦ï¼ˆ ç‹—å¤´
   - è‡³äºŽ bert-xlarge ä¸ workï¼Œ è€Œ ALBert-xlarge workï¼Œæˆ‘çš„æ„Ÿè§‰æ˜¯é¦–å…ˆ Embed å±‚ç»´åº¦è¢«æ”¾å¤§å¤ªå¤šäº†ï¼Œå¯¼è‡´ä¿¡æ¯è¿‡äºŽå‘ä¸Žä¸Šä¸‹æ–‡åç§»å§. è€Œå¢žåŠ  Hidden size ä¼šä½¿å¾—é åŽçš„ layer ä¿¡æ¯è¿›ä¸€æ­¥è¢«æ”¾å¤§. è€Œ ALBer-xlarge é¦–å…ˆå‡å° Embed ç»´åº¦è¿‡å¤§çš„å½±å“, ç„¶åŽ share äº†æ‰€æœ‰å±‚çš„å‚æ•°, ç›¸å½“äºŽæŠŠåŽŸæ¥æ‰€æœ‰ layer åšäº†ä¸ªå¹³å‡(è€Œä¸æ˜¯åªå–æœ€åŽä¸€å±‚ hidden layer çš„è¾“å‡º. è¿™æ ·å®žé™…ä¸Šä¼šå‰Šå¼±è¿‡äºŽåƒæ›´é«˜å±‚è¯­ä¹‰ä¿¡æ¯åç§»çš„è¶‹åŠ¿. æ„Ÿè§‰å¦‚æžœä½œè€…åšäº† ALBert-xlarge çš„ no-share å®žéªŒåº”è¯¥ä¼šæœ‰æ‰€ä½“çŽ°.
   - è€Œ ALBert-xxlarge æ¯” ALBert-xlarge æ›´ work, æ„Ÿè§‰ä¹Ÿæ˜¯å·®ä¸å¤šçš„åŽŸå› . å¢žåŠ  Hidden size æ˜¯æ‰©å……é åŽå±‚çº§èŽ·å¾—çš„ semantic ä¿¡æ¯. ä¸»è¦æ˜¯å› ä¸ºåšäº†å‚æ•°å…±äº«,å®žé™…ä¸Šä½œç”¨åˆ°è¾“å‡ºçš„æ˜¯ layer çš„æŸç§å¹³å‡. ALBert-xlarge æ¯” ALbert-large work, ALBert-xxlarge æ¯” ALbert-xlarge work è¯´æ˜Žå¯¹ ALBert è¿™ç§ Language Model æ‰€è•´å«çš„ä¿¡æ¯é‡è¿˜èƒ½é€šè¿‡æ”¾å¤§ hidden size æ‰©å…….( è¿˜æ˜¯è§‰å¾—å°‘äº†ä¸€ç»„ 12 layer 2048 hidden size çš„å®žéªŒ, çŽ°åœ¨è¿™ç§æƒ…å†µåªèƒ½ç›²çŒœ
   - æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ç¯‡ Betrology é›†å¤§æˆçš„ paperï¼Œæ–‡ç« é€»è¾‘å®žéªŒéƒ½å†™çš„ä¸é”™ï¼ŒåŒ…æ‹¬ä¸€äº› idea çš„æå‡ºï¼Œå‚è€ƒæ–‡çŒ®çš„é€‰æ‹©ï¼Œè¿˜æ˜¯å»ºè®®ä¸€çœ‹ï¼Œèƒ½åŠ æ·±å¯¹ Bertology çš„ç†è§£ã€‚ï¼ˆè¯è¯´ refer é‡Œé¢è¿˜æœ‰ 10 å¤šç¯‡æ²¡çœ‹è¿‡ è¦åŽ»é¢å£äº†

2. [**Unified Language Model Pre-training for Natural Language Understanding and Generation**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/UniLM.pdf) [-] _Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon_.
   - three mask
     - unidirectional
     - bidirectional
     - seq2seq
   - framework support both NLU & NLG
3. [**Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/T5.pdf) [-] _Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu._
   - Natural Language Decathlon.
   - every task -> seq2seq.
     - (TaskName: Sentences) -> (Task result)
     - except regression task -> (1, 1.2, 1.4 ... 5) multi-class classification
   - C4 Colossal Clean Crawled Corpus. by some heuristic method.
   - using teacher forcing + cross-entropy loss.
   - AdaFactor Optimizer + greedy decoding.
   - pack multi seq -> each entity of one batch.
   - inverse square root warm-up strategy $1/sqrt(max(n, k))$
   - SentencePrice -> 32000 vocab
   - like UnifiedLM Architectures -> Prefix LM
   - Test for multi corruption rate & corrupted span lens.
4. [**BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/BART.pdf) [-] _Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer_.
   - Auto-ENcode + Auto-Regressive
   - add more noisy to text
   - first, corrupting text with an arbitrary noising function.
   - second, reconstruct the original text.
   - Token Mask + Token Deletion + Text Infilling + Sentence Permutation + Document Rotation
   - LM + Permutation LM + MLM + Multitask MLM + Masked Seq-seq
   - good at NLG
5. [**ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators **](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/ELECTRA.pdf) [ICLR 2020] _Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning_.
   - Motivation: more difficult MASK
   - a little smaller MLM -> Generate random MASK -> a result
   - a discriminator if the result is correct
   - Weight Sharing -> sharing embedding & weight
   - Smaller generators
   - Training Algorithm: frozen.

## Multi-modality Bert

1. [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/Bertology/CodeBERT.pdf) [-] _Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou_.
   - PL + NL
   - MLM + RDT(Replaced Token Detection)
