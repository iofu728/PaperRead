# Bert Distilled

1. [**TinyBERT: Distilling BERT for Natural Language Understanding**](https://github.com/iofu728/PaperRead/blob/master/paper/NLP/BertDistilled/TinyBert.pdf) [submit ICLR 2020] _Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu_.
   - New Two-stage distilling framework
     - pre-training Distillations
     - task-specific Distillations
   - Multi-grain distilling
     - Transformer-layer Distillations
       - Attention distillations
       - Hidden Distillations
     - Embedding-layer Distillations
     - Prediction-layer Distillations
