# Parallelism

1. [**Megatron-LM: Training Multi-Billion Parameter Language Models Using GPU Model Parallelism**](https://github.com/iofu728/PaperRead/blob/master/NLP/Parallelism/Megatron-LM.pdf) [-] _Mohammad Shoeybi, Mostofa Patwary Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro_.
   - parallelism bert. 
   - using Pre-LN Transformer instead of Post-LN Transformer in Origin Bert 
   - using the GELU instead of RELU
   - little of code change in parallelism architecture.
